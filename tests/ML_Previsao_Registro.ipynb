# ============================================================
# NOTEBOOK 2: MODELAGEM, TREINAMENTO E AVALIAÃ‡ÃƒO
# Dataset: Product Sentiment Classification
# Autor: Pedro Morato Lahoz
# ============================================================

# === CÃ‰LULA 1: INSTALAÃ‡ÃƒO E IMPORTS ===
!pip install -q vaderSentiment transformers torch scikit-learn azure-ai-ml azure-identity

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, classification_report, confusion_matrix)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
%matplotlib inline

print("âœ… Bibliotecas importadas!")

# === CÃ‰LULA 2: CARREGAR DATASET TRATADO ===
print("ðŸ“¥ Carregando dataset tratado...")

df = pd.read_csv('dataset_tratado.csv')

print(f"âœ… Dataset carregado!")
print(f"ðŸ“Š Shape: {df.shape}")
print(f"ðŸ“‹ Colunas: {df.columns.tolist()}")

df.head()

# === CÃ‰LULA 3: PREPARAÃ‡ÃƒO DOS DADOS ===
print("=" * 60)
print("ðŸ”§ PREPARAÃ‡ÃƒO PARA MODELAGEM")
print("=" * 60)

# Separar features e target
X = df['text_clean']
y = df['sentiment']

# Split treino/teste (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nðŸ“Š DistribuiÃ§Ã£o dos dados:")
print(f"   Treino: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)")
print(f"   Teste:  {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)")

print(f"\nðŸ“ˆ DistribuiÃ§Ã£o de classes (Treino):")
print(y_train.value_counts())
print(f"\n{(y_train.value_counts() / len(y_train) * 100).round(2)}")

# === CÃ‰LULA 4: VETORIZAÃ‡ÃƒO TF-IDF ===
print("=" * 60)
print("ðŸ”¢ VETORIZAÃ‡ÃƒO TF-IDF")
print("=" * 60)

print("\nâš™ï¸  Criando vetorizador TF-IDF...")
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),
    min_df=5,
    max_df=0.8
)

print("ðŸ”„ Transformando textos em vetores...")
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"\nâœ… VetorizaÃ§Ã£o concluÃ­da!")
print(f"   Shape treino: {X_train_tfidf.shape}")
print(f"   Shape teste:  {X_test_tfidf.shape}")
print(f"   VocabulÃ¡rio:  {len(vectorizer.vocabulary_):,} palavras")

# === CÃ‰LULA 5: MODELO 1 - VADER (Baseline) ===
print("=" * 60)
print("ðŸ¤– MODELO 1: VADER (Baseline)")
print("=" * 60)

analyzer = SentimentIntensityAnalyzer()

def vader_predict(text):
    """Prediz sentimento usando VADER"""
    scores = analyzer.polarity_scores(text)
    compound = scores['compound']
    
    if compound >= 0.05:
        return 'positive'
    elif compound <= -0.05:
        return 'negative'
    else:
        return 'neutral'

print("ðŸ”„ Fazendo prediÃ§Ãµes com VADER...")
y_pred_vader = X_test.apply(vader_predict)

# Avaliar
acc_vader = accuracy_score(y_test, y_pred_vader)
print(f"\nâœ… VADER - AcurÃ¡cia: {acc_vader:.4f} ({acc_vader*100:.2f}%)")

print("\nðŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
print(classification_report(y_test, y_pred_vader))

# === CÃ‰LULA 6: MODELO 2 - NAIVE BAYES ===
print("=" * 60)
print("ðŸ¤– MODELO 2: NAIVE BAYES")
print("=" * 60)

print("âš™ï¸  Treinando Multinomial Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, y_train)

print("ðŸ”„ Fazendo prediÃ§Ãµes...")
y_pred_nb = nb_model.predict(X_test_tfidf)

# Avaliar
acc_nb = accuracy_score(y_test, y_pred_nb)
print(f"\nâœ… Naive Bayes - AcurÃ¡cia: {acc_nb:.4f} ({acc_nb*100:.2f}%)")

print("\nðŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
print(classification_report(y_test, y_pred_nb))

# === CÃ‰LULA 7: MODELO 3 - LOGISTIC REGRESSION ===
print("=" * 60)
print("ðŸ¤– MODELO 3: LOGISTIC REGRESSION")
print("=" * 60)

print("âš™ï¸  Treinando Logistic Regression...")
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train_tfidf, y_train)

print("ðŸ”„ Fazendo prediÃ§Ãµes...")
y_pred_lr = lr_model.predict(X_test_tfidf)

# Avaliar
acc_lr = accuracy_score(y_test, y_pred_lr)
print(f"\nâœ… Logistic Regression - AcurÃ¡cia: {acc_lr:.4f} ({acc_lr*100:.2f}%)")

print("\nðŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
print(classification_report(y_test, y_pred_lr))

# === CÃ‰LULA 8: MODELO 4 - RANDOM FOREST ===
print("=" * 60)
print("ðŸ¤– MODELO 4: RANDOM FOREST")
print("=" * 60)

print("âš™ï¸  Treinando Random Forest...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train_tfidf, y_train)

print("ðŸ”„ Fazendo prediÃ§Ãµes...")
y_pred_rf = rf_model.predict(X_test_tfidf)

# Avaliar
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"\nâœ… Random Forest - AcurÃ¡cia: {acc_rf:.4f} ({acc_rf*100:.2f}%)")

print("\nðŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
print(classification_report(y_test, y_pred_rf))

# === CÃ‰LULA 9: COMPARAÃ‡ÃƒO DE MODELOS ===
print("=" * 60)
print("ðŸ“Š COMPARAÃ‡ÃƒO DE TODOS OS MODELOS")
print("=" * 60)

# Calcular mÃ©tricas para todos
models = {
    'VADER': y_pred_vader,
    'Naive Bayes': y_pred_nb,
    'Logistic Regression': y_pred_lr,
    'Random Forest': y_pred_rf
}

results = []
for name, predictions in models.items():
    acc = accuracy_score(y_test, predictions)
    prec = precision_score(y_test, predictions, average='weighted', zero_division=0)
    rec = recall_score(y_test, predictions, average='weighted', zero_division=0)
    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)
    
    results.append({
        'Modelo': name,
        'AcurÃ¡cia': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1
    })

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('AcurÃ¡cia', ascending=False)

print("\nðŸ“ˆ Tabela de Resultados:")
print(results_df.to_string(index=False))

# Visualizar
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['AcurÃ¡cia', 'Precision', 'Recall', 'F1-Score']
colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']

for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]
    results_df.plot(x='Modelo', y=metric, kind='bar', ax=ax, 
                    color=colors[idx], legend=False)
    ax.set_title(f'{metric} por Modelo', fontsize=14, fontweight='bold')
    ax.set_ylabel(metric)
    ax.set_ylim([0, 1])
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    
    # Adicionar valores nas barras
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f')

plt.tight_layout()
plt.show()

# === CÃ‰LULA 10: MATRIZES DE CONFUSÃƒO ===
print("=" * 60)
print("ðŸŽ¯ MATRIZES DE CONFUSÃƒO")
print("=" * 60)

fig, axes = plt.subplots(2, 2, figsize=(16, 14))

for idx, (name, predictions) in enumerate(models.items()):
    ax = axes[idx // 2, idx % 2]
    
    cm = confusion_matrix(y_test, predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=sorted(y_test.unique()),
                yticklabels=sorted(y_test.unique()))
    ax.set_title(f'Matriz de ConfusÃ£o - {name}', fontsize=12, fontweight='bold')
    ax.set_ylabel('Real')
    ax.set_xlabel('Predito')

plt.tight_layout()
plt.show()

# === CÃ‰LULA 11: SELEÃ‡ÃƒO DO MELHOR MODELO ===
best_model_name = results_df.iloc[0]['Modelo']
best_accuracy = results_df.iloc[0]['AcurÃ¡cia']

print("=" * 60)
print("ðŸ† MELHOR MODELO")
print("=" * 60)
print(f"\nâœ¨ Modelo selecionado: {best_model_name}")
print(f"ðŸ“ˆ AcurÃ¡cia: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")

# Selecionar modelo treinado
if best_model_name == 'Naive Bayes':
    best_model = nb_model
elif best_model_name == 'Logistic Regression':
    best_model = lr_model
elif best_model_name == 'Random Forest':
    best_model = rf_model
else:
    best_model = None
    print("\nâš ï¸  VADER nÃ£o precisa ser salvo (rule-based)")

# === CÃ‰LULA 12: ANÃLISE DE ERROS ===
print("=" * 60)
print("ðŸ” ANÃLISE DE ERROS DO MELHOR MODELO")
print("=" * 60)

if best_model_name != 'VADER':
    y_pred_best = best_model.predict(X_test_tfidf)
else:
    y_pred_best = y_pred_vader

# Criar DataFrame com resultados
results_analysis = pd.DataFrame({
    'text': X_test.values,
    'real': y_test.values,
    'predicted': y_pred_best
})

results_analysis['correct'] = results_analysis['real'] == results_analysis['predicted']

# Exemplos de acertos
print("\nâœ… EXEMPLOS DE ACERTOS:")
print("=" * 60)
correct_samples = results_analysis[results_analysis['correct']].sample(min(5, len(results_analysis[results_analysis['correct']])))
for idx, row in correct_samples.iterrows():
    print(f"\nðŸ“ Texto: {row['text'][:100]}...")
    print(f"   Real: {row['real']} | Predito: {row['predicted']} âœ“")

# Exemplos de erros
print("\n\nâŒ EXEMPLOS DE ERROS:")
print("=" * 60)
error_samples = results_analysis[~results_analysis['correct']].sample(min(5, len(results_analysis[~results_analysis['correct']])))
for idx, row in error_samples.iterrows():
    print(f"\nðŸ“ Texto: {row['text'][:100]}...")
    print(f"   Real: {row['real']} | Predito: {row['predicted']} âœ—")

# EstatÃ­sticas de erros
error_rate = (~results_analysis['correct']).sum() / len(results_analysis) * 100
print(f"\n\nðŸ“Š Taxa de erro: {error_rate:.2f}%")
print(f"ðŸ“Š Taxa de acerto: {100-error_rate:.2f}%")

# === CÃ‰LULA 13: REGISTRAR MODELO NO AZURE ML ===
print("=" * 60)
print("â˜ï¸ REGISTRO DO MODELO NO AZURE ML")
print("=" * 60)

try:
    from azure.ai.ml import MLClient
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml.entities import Model
    from azure.ai.ml.constants import AssetTypes
    import joblib
    import os
    
    # Configurar credenciais Azure
    print("\nðŸ” Configurando credenciais Azure...")
    print("âš ï¸  Certifique-se de estar autenticado no Azure CLI")
    
    # Seus dados do Azure (SUBSTITUA com seus valores)
    SUBSCRIPTION_ID = "your-subscription-id"  # â† ALTERE AQUI
    RESOURCE_GROUP = "your-resource-group"    # â† ALTERE AQUI
    WORKSPACE_NAME = "your-workspace-name"    # â† ALTERE AQUI
    
    # Criar cliente ML
    credential = DefaultAzureCredential()
    ml_client = MLClient(
        credential=credential,
        subscription_id=SUBSCRIPTION_ID,
        resource_group_name=RESOURCE_GROUP,
        workspace_name=WORKSPACE_NAME
    )
    
    print(f"âœ… Conectado ao workspace: {WORKSPACE_NAME}")
    
    # Salvar modelo localmente primeiro
    print("\nðŸ’¾ Salvando modelo localmente...")
    model_dir = "./model"
    os.makedirs(model_dir, exist_ok=True)
    
    if best_model_name != 'VADER':
        # Salvar modelo ML
        joblib.dump(best_model, f"{model_dir}/model.pkl")
        joblib.dump(vectorizer, f"{model_dir}/vectorizer.pkl")
        print(f"   âœ… Modelo salvo: {model_dir}/model.pkl")
        print(f"   âœ… Vectorizer salvo: {model_dir}/vectorizer.pkl")
        
        # Salvar informaÃ§Ãµes do modelo
        model_info = {
            'model_name': best_model_name,
            'accuracy': float(best_accuracy),
            'features': X_train_tfidf.shape[1],
            'classes': list(y_train.unique())
        }
        import json
        with open(f"{model_dir}/model_info.json", 'w') as f:
            json.dump(model_info, f, indent=2)
        print(f"   âœ… Info salva: {model_dir}/model_info.json")
    
    # Registrar no Azure ML
    print("\nâ˜ï¸  Registrando modelo no Azure ML...")
    
    model = Model(
        path=model_dir,
        type=AssetTypes.CUSTOM_MODEL,
        name=f"sentiment-analysis-{best_model_name.lower().replace(' ', '-')}",
        description=f"Modelo de anÃ¡lise de sentimentos usando {best_model_name}",
        tags={
            "model_type": best_model_name,
            "accuracy": str(best_accuracy),
            "framework": "scikit-learn",
            "task": "sentiment-analysis"
        }
    )
    
    registered_model = ml_client.models.create_or_update(model)
    
    print(f"\nâœ… Modelo registrado no Azure ML!")
    print(f"   ðŸ“¦ Nome: {registered_model.name}")
    print(f"   ðŸ”¢ VersÃ£o: {registered_model.version}")
    print(f"   ðŸ“Š AcurÃ¡cia: {best_accuracy:.4f}")
    
except ImportError:
    print("\nâš ï¸  Bibliotecas Azure nÃ£o instaladas")
    print("   Execute: pip install azure-ai-ml azure-identity")
    print("\nðŸ’¾ Salvando modelo localmente apenas...")
    
    import joblib
    import os
    
    model_dir = "./model"
    os.makedirs(model_dir, exist_ok=True)
    
    if best_model_name != 'VADER':
        joblib.dump(best_model, f"{model_dir}/model.pkl")
        joblib.dump(vectorizer, f"{model_dir}/vectorizer.pkl")
        print(f"   âœ… Modelo salvo: {model_dir}/model.pkl")
        print(f"   âœ… Vectorizer salvo: {model_dir}/vectorizer.pkl")
    
except Exception as e:
    print(f"\nâŒ Erro ao registrar no Azure: {str(e)}")
    print("\nðŸ’¾ Salvando modelo localmente apenas...")
    
    import joblib
    import os
    
    model_dir = "./model"
    os.makedirs(model_dir, exist_ok=True)
    
    if best_model_name != 'VADER':
        joblib.dump(best_model, f"{model_dir}/model.pkl")
        joblib.dump(vectorizer, f"{model_dir}/vectorizer.pkl")
        print(f"   âœ… Modelo salvo: {model_dir}/model.pkl")

# === CÃ‰LULA 14: FUNÃ‡ÃƒO DE PREDIÃ‡ÃƒO ===
print("=" * 60)
print("ðŸŽ¯ CRIANDO FUNÃ‡ÃƒO DE PREDIÃ‡ÃƒO")
print("=" * 60)

def predict_sentiment(text, model_name=best_model_name):
    """
    Prediz sentimento de um texto
    
    Args:
        text (str): Texto para anÃ¡lise
        model_name (str): Nome do modelo a usar
    
    Returns:
        dict: Resultado com sentimento e confianÃ§a
    """
    # Limpar texto
    text_clean = text.lower()
    text_clean = re.sub(r'http\S+', '', text_clean)
    text_clean = re.sub(r'[^a-z\s]', '', text_clean)
    text_clean = ' '.join(text_clean.split())
    
    if model_name == 'VADER':
        scores = analyzer.polarity_scores(text_clean)
        compound = scores['compound']
        
        if compound >= 0.05:
            sentiment = 'positive'
        elif compound <= -0.05:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        confidence = abs(compound)
        
    else:
        # Vetorizar
        text_vec = vectorizer.transform([text_clean])
        
        # PrediÃ§Ã£o
        sentiment = best_model.predict(text_vec)[0]
        
        # ConfianÃ§a (probabilidade)
        if hasattr(best_model, 'predict_proba'):
            probs = best_model.predict_proba(text_vec)[0]
            confidence = max(probs)
        else:
            confidence = 1.0
    
    return {
        'text': text,
        'sentiment': sentiment,
        'confidence': confidence,
        'model': model_name
    }

print("\nâœ… FunÃ§Ã£o de prediÃ§Ã£o criada!")

# === CÃ‰LULA 15: TESTAR MODELO ===
print("=" * 60)
print("ðŸ§ª TESTANDO MODELO COM EXEMPLOS")
print("=" * 60)

test_examples = [
    "This product is amazing! I love it so much!",
    "Terrible quality. Waste of money. Very disappointed.",
    "It's okay, nothing special but does the job.",
    "Best purchase ever! Highly recommend to everyone!",
    "Horrible experience. Would not buy again."
]

print(f"\nðŸ¤– Modelo: {best_model_name}\n")

for text in test_examples:
    result = predict_sentiment(text)
    
    emoji = "ðŸ˜Š" if result['sentiment'] == 'positive' else "ðŸ˜ž" if result['sentiment'] == 'negative' else "ðŸ˜"
    
    print(f"\n{'='*60}")
    print(f"ðŸ“ Texto: {text}")
    print(f"{emoji} Sentimento: {result['sentiment'].upper()}")
    print(f"ðŸ“Š ConfianÃ§a: {result['confidence']:.2%}")

# === CÃ‰LULA 16: ANÃLISE DE IMPORTÃ‚NCIA DE FEATURES ===
if best_model_name in ['Logistic Regression', 'Random Forest']:
    print("\n" + "=" * 60)
    print("ðŸ” ANÃLISE DE IMPORTÃ‚NCIA DE FEATURES")
    print("=" * 60)
    
    feature_names = vectorizer.get_feature_names_out()
    
    if best_model_name == 'Logistic Regression':
        # Para cada classe
        for idx, sentiment in enumerate(best_model.classes_):
            print(f"\nðŸ“Š Top 15 palavras para '{sentiment}':")
            coefficients = best_model.coef_[idx]
            top_indices = coefficients.argsort()[-15:][::-1]
            
            for i in top_indices:
                print(f"   {feature_names[i]:20s}: {coefficients[i]:.4f}")
    
    elif best_model_name == 'Random Forest':
        print("\nðŸ“Š Top 20 features mais importantes:")
        importances = best_model.feature_importances_
        top_indices = importances.argsort()[-20:][::-1]
        
        for i in top_indices:
            print(f"   {feature_names[i]:20s}: {importances[i]:.6f}")
        
        # Visualizar
        plt.figure(figsize=(12, 6))
        top_features = [(feature_names[i], importances[i]) for i in top_indices[:15]]
        features, values = zip(*top_features)
        plt.barh(features, values, color='skyblue')
        plt.xlabel('ImportÃ¢ncia')
        plt.title('Top 15 Features Mais Importantes', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

# === CÃ‰LULA 17: CRIAR SCRIPT DE INFERÃŠNCIA ===
print("=" * 60)
print("ðŸ“ CRIANDO SCRIPT DE INFERÃŠNCIA")
print("=" * 60)

inference_script = f'''
import joblib
import re

# Carregar modelo e vectorizer
model = joblib.load('model/model.pkl')
vectorizer = joblib.load('model/vectorizer.pkl')

def predict(text):
    """Prediz sentimento de um texto"""
    # Limpar texto
    text_clean = text.lower()
    text_clean = re.sub(r'http\\S+', '', text_clean)
    text_clean = re.sub(r'[^a-z\\s]', '', text_clean)
    text_clean = ' '.join(text_clean.split())
    
    # Vetorizar
    text_vec = vectorizer.transform([text_clean])
    
    # PrediÃ§Ã£o
    sentiment = model.predict(text_vec)[0]
    
    # ConfianÃ§a
    if hasattr(model, 'predict_proba'):
        probs = model.predict_proba(text_vec)[0]
        confidence = max(probs)
    else:
        confidence = 1.0
    
    return {{
        'sentiment': sentiment,
        'confidence': float(confidence)
    }}

# Exemplo de uso
if __name__ == "__main__":
    text = "This product is amazing!"
    result = predict(text)
    print(f"Text: {{text}}")
    print(f"Sentiment: {{result['sentiment']}}")
    print(f"Confidence: {{result['confidence']:.2%}}")
'''

with open('inference.py', 'w') as f:
    f.write(inference_script)

print("\nâœ… Script de inferÃªncia criado: inference.py")

# === CÃ‰LULA 18: CONCLUSÃ•ES E PRÃ“XIMOS PASSOS ===
print("\n" + "=" * 60)
print("ðŸ“ CONCLUSÃ•ES DO PROJETO")
print("=" * 60)

print(f"""
âœ… RESUMO DO PROJETO:

1ï¸âƒ£  DATASET:
   â€¢ {len(df):,} reviews processadas
   â€¢ 3 classes de sentimento (positive, negative, neutral)
   â€¢ Dataset limpo e balanceado

2ï¸âƒ£  MODELOS TESTADOS:
   â€¢ VADER (Baseline)
   â€¢ Naive Bayes
   â€¢ Logistic Regression
   â€¢ Random Forest

3ï¸âƒ£  MELHOR MODELO:
   â€¢ ðŸ† {best_model_name}
   â€¢ ðŸ“ˆ AcurÃ¡cia: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)
   â€¢ âœ… Modelo salvo e registrado

4ï¸âƒ£  RESULTADOS:
""")

print(results_df.to_string(index=False))

print(f"""

5ï¸âƒ£  ARQUIVOS GERADOS:
   â€¢ âœ… dataset_tratado.csv - Dataset limpo
   â€¢ âœ… model/model.pkl - Modelo treinado
   â€¢ âœ… model/vectorizer.pkl - Vectorizer TF-IDF
   â€¢ âœ… inference.py - Script de inferÃªncia
   â€¢ âœ… Modelo registrado no Azure ML (se configurado)

ðŸŽ¯ PRÃ“XIMOS PASSOS:

1ï¸âƒ£  MELHORIAS:
   â€¢ Testar BERT/RoBERTa para comparaÃ§Ã£o
   â€¢ Implementar ensemble de modelos
   â€¢ Fine-tuning de hiperparÃ¢metros
   â€¢ Aumentar dataset com data augmentation

2ï¸âƒ£  DEPLOYMENT:
   â€¢ Criar API Flask/FastAPI
   â€¢ Deploy no Azure App Service
   â€¢ Monitoramento de performance
   â€¢ A/B testing

3ï¸âƒ£  FEATURES ADICIONAIS:
   â€¢ AnÃ¡lise de aspectos (aspect-based sentiment)
   â€¢ DetecÃ§Ã£o de sarcasmo
   â€¢ Suporte multi-idioma
   â€¢ AnÃ¡lise de emoÃ§Ãµes (alÃ©m de sentimentos)

ðŸ’¡ APRENDIZADOS:

âœ“ Pipeline completo de ML (EDA â†’ Modelagem â†’ Deploy)
âœ“ ComparaÃ§Ã£o de mÃºltiplos modelos
âœ“ Registro e versionamento de modelos
âœ“ Boas prÃ¡ticas de ML Engineering
âœ“ IntegraÃ§Ã£o com Azure ML

""")

print("=" * 60)
print("ðŸŽ‰ PROJETO CONCLUÃDO COM SUCESSO!")
print("=" * 60)

# === CÃ‰LULA 19: MÃ‰TRICAS FINAIS PARA RELATÃ“RIO ===
print("\nðŸ“Š MÃ‰TRICAS FINAIS PARA RELATÃ“RIO:\n")

final_metrics = {
    'Dataset Original': initial_size,
    'Dataset Tratado': len(df),
    'Taxa de RetenÃ§Ã£o': f"{len(df)/initial_size*100:.2f}%",
    'Features (TF-IDF)': X_train_tfidf.shape[1],
    'Treino/Teste Split': '80/20',
    'Melhor Modelo': best_model_name,
    'AcurÃ¡cia Final': f"{best_accuracy:.4f} ({best_accuracy*100:.2f}%)",
    'Precision (weighted)': f"{results_df.iloc[0]['Precision']:.4f}",
    'Recall (weighted)': f"{results_df.iloc[0]['Recall']:.4f}",
    'F1-Score (weighted)': f"{results_df.iloc[0]['F1-Score']:.4f}"
}

for key, value in final_metrics.items():
    print(f"   {key:25s}: {value}")

print("\n" + "=" * 60)
print("âœ… NOTEBOOKS COMPLETOS - PRONTO PARA ENTREGA!")
print("=" * 60)