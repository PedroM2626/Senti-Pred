{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem - Senti-Pred\n",
    "\n",
    "Este notebook contém o desenvolvimento e treinamento dos modelos de análise de sentimentos para o projeto Senti-Pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Carregar os dados processados\n",
    "processed_path = '../data/processed/processed_data.csv'\n",
    "df = pd.read_csv(processed_path)\n",
    "\n",
    "# Exibir as primeiras linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparar dados para modelagem\n",
    "if 'text_lemmatized' in df.columns and 'sentiment' in df.columns:\n",
    "    X = df['text_lemmatized']\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    # Dividir em conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Tamanho do conjunto de treino: {X_train.shape[0]}\")\n",
    "    print(f\"Tamanho do conjunto de teste: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1: Regressão Logística com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline para Regressão Logística\n",
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "print(\"Relatório de Classificação - Regressão Logística:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=lr_pipeline.classes_, yticklabels=lr_pipeline.classes_)\n",
    "plt.title('Matriz de Confusão - Regressão Logística')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2: Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline para Naive Bayes\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred_nb = nb_pipeline.predict(X_test)\n",
    "print(\"Relatório de Classificação - Naive Bayes:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=nb_pipeline.classes_, yticklabels=nb_pipeline.classes_)\n",
    "plt.title('Matriz de Confusão - Naive Bayes')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline para Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "print(\"Relatório de Classificação - Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=rf_pipeline.classes_, yticklabels=rf_pipeline.classes_)\n",
    "plt.title('Matriz de Confusão - Random Forest')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comparar acurácia dos modelos\n",
    "models = {\n",
    "    'Regressão Logística': (lr_pipeline, y_pred_lr),\n",
    "    'Naive Bayes': (nb_pipeline, y_pred_nb),\n",
    "    'Random Forest': (rf_pipeline, y_pred_rf)\n",
    "}\n",
    "\n",
    "accuracies = {}\n",
    "for name, (model, y_pred) in models.items():\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies[name] = acc\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "# Visualizar comparação\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green', 'orange'])\n",
    "plt.title('Comparação de Acurácia entre Modelos')\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies.values()):\n",
    "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar o Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificar o melhor modelo\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "best_model = models[best_model_name][0]\n",
    "print(f\"Melhor modelo: {best_model_name} com acurácia de {accuracies[best_model_name]:.4f}\")\n",
    "\n",
    "# Salvar o modelo\n",
    "model_path = '../src/models/sentiment_model.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões da Modelagem\n",
    "\n",
    "- Resumo dos modelos testados\n",
    "- Análise do desempenho do melhor modelo\n",
    "- Próximos passos para avaliação e implantação"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}