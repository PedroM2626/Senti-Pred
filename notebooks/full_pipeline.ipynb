{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTI-PRED: Pipeline Completo de Analise de Sentimentos\n",
    "## Autor: Pedro Morato Lahoz\n",
    "## Data: Outubro 2025\n",
    "\n",
    "---\n",
    "\n",
    "### Indice:\n",
    "1. [Configuracao Inicial](#config)\n",
    "2. [Analise Exploratoria (EDA)](#eda)\n",
    "3. [Pre-processamento](#preprocessing)\n",
    "4. [Modelagem](#modeling)\n",
    "5. [Avaliacao](#evaluation)\n",
    "6. [Deploy com Docker](#deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuracao Inicial <a id='config'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacoes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configuracoes visuais\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"[OK] Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download recursos NLTK\n",
    "recursos = ['punkt', 'stopwords', 'wordnet', 'punkt_tab', 'rslp']\n",
    "for recurso in recursos:\n",
    "    nltk.download(recurso, quiet=True)\n",
    "\n",
    "print(\"[OK] Recursos NLTK baixados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar caminhos\n",
    "DATA_RAW = '../data/raw/Test.csv'\n",
    "DATA_PROCESSED = '../data/processed/processed_data.csv'\n",
    "MODEL_PATH = '../src/models/sentiment_model.pkl'\n",
    "\n",
    "# Criar diretorios se nao existirem\n",
    "os.makedirs(os.path.dirname(DATA_PROCESSED), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "os.makedirs('../reports/visualizacoes', exist_ok=True)\n",
    "\n",
    "print(\"[OK] Estrutura de diretorios configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Analise Exploratoria de Dados (EDA) <a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "print(\"Carregando dados...\")\n",
    "df = pd.read_csv(DATA_RAW)\n",
    "print(f\"[OK] Dados carregados: {df.shape[0]} registros e {df.shape[1]} colunas\")\n",
    "\n",
    "# Exibir primeiras linhas\n",
    "print(\"\\nPrimeiras linhas do dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Informações básicas\n",
    "print(\"\\nInformações do dataset:\")\n",
    "df.info()\n",
    "\n",
    "# Estatísticas descritivas\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nValores nulos por coluna:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição de classes (assumindo que a coluna de sentimento é 'sentiment')\n",
    "if 'sentiment' in df.columns:\n",
    "    print(\"\\nDistribuição de classes:\")\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    display(sentiment_counts)\n",
    "    \n",
    "    # Visualizar distribuição\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='sentiment', data=df)\n",
    "    plt.title('Distribuição de Sentimentos')\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Se não houver coluna 'sentiment', assumir que a última coluna é a target\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\"\\nDistribuição de classes (coluna {target_col}):\")\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    display(target_counts)\n",
    "    \n",
    "    # Visualizar distribuição\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=target_col, data=df)\n",
    "    plt.title(f'Distribuição de {target_col}')\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de comprimento de texto\n",
    "print(\"\\nAnálise de comprimento de texto:\")\n",
    "# Identificar a coluna de texto (assumindo que é a primeira coluna ou se chama 'text')\n",
    "text_col = 'text' if 'text' in df.columns else df.columns[0]\n",
    "\n",
    "df['text_length'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "print(f\"Comprimento médio: {df['text_length'].mean():.2f} palavras\")\n",
    "print(f\"Comprimento mínimo: {df['text_length'].min()} palavras\")\n",
    "print(f\"Comprimento máximo: {df['text_length'].max()} palavras\")\n",
    "\n",
    "# Visualizar distribuição de comprimento\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['text_length'], bins=50, kde=True)\n",
    "plt.title('Distribuição de Comprimento de Texto')\n",
    "plt.xlabel('Número de Palavras')\n",
    "plt.ylabel('Frequência')\n",
    "plt.axvline(df['text_length'].mean(), color='red', linestyle='--', label=f'Média: {df[\"text_length\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/visualizacoes/text_lenght.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras mais comuns\n",
    "print(\"\\nPalavras mais comuns:\")\n",
    "all_words = ' '.join(df[text_col].astype(str)).lower().split()\n",
    "word_counts = pd.Series(all_words).value_counts()\n",
    "print(word_counts.head(20))\n",
    "\n",
    "# Visualizar palavras mais comuns\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_counts[:20].plot(kind='bar')\n",
    "plt.title('20 Palavras Mais Frequentes')\n",
    "plt.xlabel('Palavra')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/visualizacoes/number_words.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Análise exploratória concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Pre-processamento <a id='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funções de pré-processamento\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpa o texto removendo URLs, menções, pontuações e números\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords do texto\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lematiza o texto\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "print(\"[OK] Funções de pré-processamento definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar pré-processamento\n",
    "print(\"Aplicando pré-processamento...\")\n",
    "\n",
    "# Identificar colunas\n",
    "text_col = 'text' if 'text' in df.columns else df.columns[0]\n",
    "target_col = 'sentiment' if 'sentiment' in df.columns else df.columns[-1]\n",
    "\n",
    "# Criar cópia para processamento\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Aplicar etapas de pré-processamento\n",
    "print(\"1. Limpando textos...\")\n",
    "df_processed['text_clean'] = df_processed[text_col].apply(clean_text)\n",
    "\n",
    "print(\"2. Removendo stopwords...\")\n",
    "df_processed['text_no_stop'] = df_processed['text_clean'].apply(remove_stopwords)\n",
    "\n",
    "print(\"3. Lematizando textos...\")\n",
    "df_processed['text_lemmatized'] = df_processed['text_no_stop'].apply(lemmatize_text)\n",
    "\n",
    "# Exibir exemplos\n",
    "print(\"\\nExemplos de textos processados:\")\n",
    "examples = pd.DataFrame({\n",
    "    'Original': df_processed[text_col].head(3),\n",
    "    'Limpo': df_processed['text_clean'].head(3),\n",
    "    'Sem Stopwords': df_processed['text_no_stop'].head(3),\n",
    "    'Lematizado': df_processed['text_lemmatized'].head(3)\n",
    "})\n",
    "display(examples)\n",
    "\n",
    "# Salvar dados processados\n",
    "df_processed.to_csv(DATA_PROCESSED, index=False)\n",
    "print(f\"[OK] Dados processados salvos em: {DATA_PROCESSED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modelagem <a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para modelagem\n",
    "print(\"Preparando dados para modelagem...\")\n",
    "\n",
    "# Definir features e target\n",
    "X = df_processed['text_lemmatized']\n",
    "y = df_processed[target_col]\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dados de treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Dados de teste: {X_test.shape[0]} amostras\")\n",
    "\n",
    "# Verificar distribuição nos conjuntos\n",
    "print(\"\\nDistribuição de classes:\")\n",
    "print(f\"Treino:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTeste:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir e treinar modelos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TREINAMENTO DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Definir modelos a serem testados\n",
    "models_config = {\n",
    "    'Regressão Logística': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "models = {}\n",
    "\n",
    "# Treinar e avaliar cada modelo\n",
    "for name, model in models_config.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    \n",
    "    # Criar pipeline com vetorização TF-IDF\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Treinar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predizer\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Avaliar\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"Relatório de classificação:\\n{report}\")\n",
    "    \n",
    "    # Armazenar modelo\n",
    "    models[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "print(\"\\n[OK] Treinamento de modelos concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar modelos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARAÇÃO DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extrair acurácias\n",
    "accuracies = {name: info['accuracy'] for name, info in models.items()}\n",
    "print(\"Acurácias:\")\n",
    "for name, acc in accuracies.items():\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "# Visualizar comparação\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values())\n",
    "plt.title('Comparação de Acurácia entre Modelos')\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/visualizacoes/models_comparasion.png')\n",
    "plt.show()\n",
    "\n",
    "# Identificar melhor modelo\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "best_model = models[best_model_name]['pipeline']\n",
    "print(f\"\\nMelhor modelo: {best_model_name} com acurácia de {accuracies[best_model_name]:.4f}\")\n",
    "\n",
    "# Salvar melhor modelo\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "print(f\"[OK] Melhor modelo salvo em: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Avaliacao <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação detalhada do melhor modelo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"AVALIAÇÃO DETALHADA: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Obter predições\n",
    "y_pred = models[best_model_name]['predictions']\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y), \n",
    "            yticklabels=np.unique(y))\n",
    "plt.title(f'Matriz de Confusão - {best_model_name}')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../reports/visualizacoes/{best_model_name.lower().replace(\" \", \"-\")}_matriz.png')\n",
    "plt.show()\n",
    "\n",
    "# Relatório de classificação\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Verificar se o modelo suporta predict_proba\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    # Curva ROC (para problemas binários)\n",
    "    if len(np.unique(y)) == 2:\n",
    "        y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taxa de Falsos Positivos')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "        plt.title('Curva ROC')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Curva Precision-Recall\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, color='blue', lw=2)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Curva Precision-Recall')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/visualizacoes/precision.png')\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n[OK] Avaliação detalhada concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com exemplos reais\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTE COM EXEMPLOS REAIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carregar modelo\n",
    "loaded_model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Exemplos de textos para teste\n",
    "exemplos = [\n",
    "    \"Adorei o produto, superou minhas expectativas!\",\n",
    "    \"Produto de péssima qualidade, não recomendo.\",\n",
    "    \"Entrega rápida, mas o produto veio com defeito.\",\n",
    "    \"Atendimento ao cliente excelente, resolveram meu problema rapidamente.\"\n",
    "]\n",
    "\n",
    "# Processar exemplos\n",
    "exemplos_processados = []\n",
    "for texto in exemplos:\n",
    "    texto_limpo = clean_text(texto)\n",
    "    texto_sem_stop = remove_stopwords(texto_limpo)\n",
    "    texto_lemmatizado = lemmatize_text(texto_sem_stop)\n",
    "    exemplos_processados.append(texto_lemmatizado)\n",
    "\n",
    "# Predizer\n",
    "predicoes = loaded_model.predict(exemplos_processados)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nResultados:\")\n",
    "for i, (texto, pred) in enumerate(zip(exemplos, predicoes)):\n",
    "    print(f\"Exemplo {i+1}: {texto}\")\n",
    "    print(f\"Predição: {pred}\\n\")\n",
    "\n",
    "print(\"\\n[OK] Teste com exemplos reais concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deploy com Docker <a id='deploy'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo Dockerfile para API\n",
    "dockerfile_content = '''\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n",
    "'''\n",
    "\n",
    "# Escrever Dockerfile\n",
    "with open('../src/api/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"[OK] Dockerfile criado em: ../src/api/Dockerfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo docker-compose.yml\n",
    "docker_compose_content = '''\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: ./src/api/Dockerfile\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ./:/app\n",
    "    environment:\n",
    "      - DEBUG=True\n",
    "'''\n",
    "\n",
    "# Escrever docker-compose.yml\n",
    "with open('../docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(\"[OK] docker-compose.yml criado em: ../docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo de views para API\n",
    "views_content = '''\n",
    "import joblib\n",
    "import os\n",
    "from django.http import JsonResponse\n",
    "from django.views.decorators.csrf import csrf_exempt\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Carregar modelo\n",
    "MODEL_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'src', 'models', 'sentiment_model.pkl')\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Baixar recursos NLTK\n",
    "recursos = ['punkt', 'stopwords', 'wordnet', 'punkt_tab', 'rslp']\n",
    "for recurso in recursos:\n",
    "    nltk.download(recurso, quiet=True)\n",
    "\n",
    "# Funções de pré-processamento\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text_clean = clean_text(text)\n",
    "    text_no_stop = remove_stopwords(text_clean)\n",
    "    text_lemmatized = lemmatize_text(text_no_stop)\n",
    "    return text_lemmatized\n",
    "\n",
    "@csrf_exempt\n",
    "def predict_sentiment(request):\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            data = json.loads(request.body)\n",
    "            text = data.get('text', '')\n",
    "            \n",
    "            # Pré-processar texto\n",
    "            processed_text = preprocess_text(text)\n",
    "            \n",
    "            # Predizer sentimento\n",
    "            prediction = model.predict([processed_text])[0]\n",
    "            \n",
    "            # Retornar resultado\n",
    "            return JsonResponse({\n",
    "                'text': text,\n",
    "                'sentiment': prediction,\n",
    "                'success': True\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return JsonResponse({\n",
    "                'error': str(e),\n",
    "                'success': False\n",
    "            }, status=400)\n",
    "    \n",
    "    return JsonResponse({\n",
    "        'error': 'Método não permitido',\n",
    "        'success': False\n",
    "    }, status=405)\n",
    "'''\n",
    "\n",
    "# Escrever views.py\n",
    "with open('../src/api/views.py', 'w') as f:\n",
    "    f.write(views_content)\n",
    "\n",
    "print(\"[OK] views.py criado em: ../src/api/views.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo de URLs para API\n",
    "urls_content = '''\n",
    "from django.urls import path\n",
    "from .views import predict_sentiment\n",
    "\n",
    "urlpatterns = [\n",
    "    path('predict/', predict_sentiment, name='predict_sentiment'),\n",
    "]\n",
    "'''\n",
    "\n",
    "# Escrever urls.py\n",
    "with open('../src/api/urls.py', 'w') as f:\n",
    "    f.write(urls_content)\n",
    "\n",
    "print(\"[OK] urls.py criado em: ../src/api/urls.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo README.md\n",
    "readme_content = '''\n",
    "# SENTI-PRED: Análise de Sentimentos\n",
    "\n",
    "## Descrição\n",
    "Sistema de análise de sentimentos para textos em português, utilizando técnicas de processamento de linguagem natural e aprendizado de máquina.\n",
    "\n",
    "## Estrutura do Projeto\n",
    "```\n",
    "├── data/\n",
    "│   ├── processed/       # Dados processados\n",
    "│   └── raw/             # Dados brutos\n",
    "├── notebooks/           # Jupyter notebooks\n",
    "├── reports/             # Relatórios e visualizações\n",
    "│   └── visualizacoes/   # Gráficos e visualizações\n",
    "├── src/                 # Código fonte\n",
    "│   ├── api/             # API para deploy\n",
    "│   ├── models/          # Modelos treinados\n",
    "│   └── scripts/         # Scripts de processamento\n",
    "├── tests/               # Testes\n",
    "├── docker-compose.yml   # Configuração Docker Compose\n",
    "└── README.md            # Este arquivo\n",
    "```\n",
    "\n",
    "## Instalação\n",
    "1. Clone o repositório\n",
    "2. Instale as dependências: `pip install -r requirements.txt`\n",
    "3. Execute o notebook `notebooks/full_pipeline.ipynb` para treinar o modelo\n",
    "\n",
    "## Deploy com Docker\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "## API\n",
    "Endpoint: `/api/predict/`\n",
    "\n",
    "Exemplo de requisição:\n",
    "```json\n",
    "{\n",
    "    \"text\": \"Adorei o produto, superou minhas expectativas!\"\n",
    "}\n",
    "```\n",
    "\n",
    "Exemplo de resposta:\n",
    "```json\n",
    "{\n",
    "    \"text\": \"Adorei o produto, superou minhas expectativas!\",\n",
    "    \"sentiment\": \"positivo\",\n",
    "    \"success\": true\n",
    "}\n",
    "```\n",
    "'''\n",
    "\n",
    "# Escrever README.md\n",
    "with open('../README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"[OK] README.md criado em: ../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETO FINALIZADO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nO pipeline de análise de sentimentos foi concluído com sucesso!\")\n",
    "print(\"\\nArquivos gerados:\")\n",
    "print(f\"- Dados processados: {DATA_PROCESSED}\")\n",
    "print(f\"- Modelo treinado: {MODEL_PATH}\")\n",
    "print(\"- Visualizações: ../reports/visualizacoes/\")\n",
    "print(\"- Arquivos para deploy: ../src/api/\")\n",
    "print(\"\\nPara iniciar a API, execute: docker-compose up -d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}