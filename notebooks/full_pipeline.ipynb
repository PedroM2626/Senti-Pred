{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senti-Pred: Pipeline Completo de Análise de Sentimentos\n",
    "## Autor: Pedro Morato Lahoz\n",
    "\n",
    "---\n",
    "\n",
    "### Indice:\n",
    "1. [Configuração Inicial](#config)\n",
    "2. [Análise Exploratória (EDA)](#eda)\n",
    "3. [Pré-processamento](#preprocessing)\n",
    "4. [Modelagem](#modeling)\n",
    "5. [Avaliação](#evaluation)\n",
    "6. [Deploy com Docker](#deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuração Inicial <a id='config'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não detectado Colab — assumindo execução local. Verifique dependências manualmente.\n",
      "Setup concluído. IN_COLAB= False\n"
     ]
    }
   ],
   "source": [
    "# Célula de setup para execução no Google Colab\n",
    "# - Clona o repositório (se necessário)\n",
    "# - Instala dependências a partir de requirements.txt\n",
    "# - Cria pastas esperadas e baixa recursos NLTK\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "REPO = 'PedroM2626/Senti-Pred'\n",
    "CLONE_DIR = Path('/content/Senti-Pred')\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not CLONE_DIR.exists():\n",
    "        print('Clonando repositório do GitHub...')\n",
    "        get_ipython().system(f'git clone https://github.com/{REPO}.git {CLONE_DIR}')\n",
    "    %cd /content/Senti-Pred\n",
    "    print('Instalando dependências (requirements.txt)...')\n",
    "    get_ipython().system('pip install -q -r requirements.txt || true')\n",
    "else:\n",
    "    print('Não detectado Colab — assumindo execução local. Verifique dependências manualmente.')\n",
    "\n",
    "# Garantir diretórios usados pelo notebook\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('reports/visualizacoes', exist_ok=True)\n",
    "os.makedirs('reports/metrics', exist_ok=True)\n",
    "os.makedirs('src/models', exist_ok=True)\n",
    "\n",
    "# NLTK resources (English)\n",
    "import nltk\n",
    "nltk_resources = ['punkt','stopwords','wordnet','omw-1.4','averaged_perceptron_tagger']\n",
    "for r in nltk_resources:\n",
    "    try:\n",
    "        nltk.download(r, quiet=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('Setup concluído. IN_COLAB=', IN_COLAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Configuracao inicial pronta — BASE_DIR: /workspaces/Senti-Pred/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from pathlib import Path\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Definir BASE_DIR de forma robusta para notebooks (Colab) e scripts\n",
    "# - Se estiver no Colab e o repositório foi clonado em /content/Senti-Pred, use esse path\n",
    "# - Caso contrário, tente usar __file__ (scripts) e, por fim, o cwd\n",
    "try:\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "except Exception:\n",
    "    # Se existe variável IN_COLAB (definida pela célula de setup), respeite-a\n",
    "    if 'IN_COLAB' in globals() and IN_COLAB:\n",
    "        candidate = Path('/content/Senti-Pred')\n",
    "        if candidate.exists():\n",
    "            BASE_DIR = str(candidate.resolve())\n",
    "        else:\n",
    "            # fallback para diretório atual (normalmente /content quando não clonado)\n",
    "            BASE_DIR = str(Path.cwd())\n",
    "    else:\n",
    "        # ambiente local/interativo sem __file__\n",
    "        BASE_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Garantir que BASE_DIR exista; se não, redefinir para cwd\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "TRAIN_RAW = os.path.join(BASE_DIR, 'data', 'raw', 'twitter_training.csv')\n",
    "VAL_RAW = os.path.join(BASE_DIR, 'data', 'raw', 'twitter_validation.csv')\n",
    "VIS_DIR = os.path.join(BASE_DIR, 'reports', 'visualizacoes')\n",
    "METRICS_DIR = os.path.join(BASE_DIR, 'reports', 'metrics')\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'src', 'models', 'sentiment_model.pkl')\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "# NLTK resources (English)\n",
    "nltk_resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4', 'averaged_perceptron_tagger']\n",
    "for r in nltk_resources:\n",
    "    try:\n",
    "        nltk.download(r, quiet=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "print('[OK] Configuracao inicial pronta — BASE_DIR:', BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Análise Exploratória (EDA) <a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados: train=74682 | validation=1000 | total=75682\n",
      "[OK] EDA concluída — imagens em reports/visualizacoes\n",
      "[OK] EDA concluída — imagens em reports/visualizacoes\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados (assumindo formato esperado)\n",
    "if not os.path.exists(TRAIN_RAW) or not os.path.exists(VAL_RAW):\n",
    "    raise FileNotFoundError(f\"Esperado arquivos em '{TRAIN_RAW}' e '{VAL_RAW}' — coloque os CSVs em data/raw/\")\n",
    "cols = ['tweet_id', 'entity', 'sentiment', 'text']\n",
    "df_train = pd.read_csv(TRAIN_RAW, names=cols, header=None, engine='python', encoding='utf-8')\n",
    "df_val = pd.read_csv(VAL_RAW, names=cols, header=None, engine='python', encoding='utf-8')\n",
    "df_train['split'] = 'train'\n",
    "df_val['split'] = 'validation'\n",
    "df = pd.concat([df_train, df_val], ignore_index=True)\n",
    "print(f\"Dados carregados: train={len(df_train)} | validation={len(df_val)} | total={len(df)}\")\n",
    "# EDA: text length distribution\n",
    "text_col = 'text'\n",
    "df['text_length'] = df[text_col].astype(str).apply(lambda s: len(s.split()))\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df['text_length'], bins=40, kde=True)\n",
    "plt.title('Distribuição de comprimento de texto')\n",
    "plt.xlabel('Número de palavras')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIS_DIR, 'text_length.png'))\n",
    "plt.close()\n",
    "# top words (raw)\n",
    "all_words = ' '.join(df[text_col].astype(str)).lower().split()\n",
    "top_raw = pd.Series(all_words).value_counts().head(20)\n",
    "plt.figure(figsize=(12,5))\n",
    "top_raw.plot(kind='bar')\n",
    "plt.title('Top words (raw)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIS_DIR, 'top_words_raw.png'))\n",
    "plt.close()\n",
    "print('[OK] EDA concluída — imagens em reports/visualizacoes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Pré-processamento <a id='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Funções de pré-processamento definidas\n",
      "[OK] Pré-processamento aplicado (dados em memória)\n",
      "[OK] Pré-processamento aplicado (dados em memória)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "def remove_stopwords_en(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "def lemmatize_text_en(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    try:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "    except Exception:\n",
    "        pos_tags = [(t, '') for t in tokens]\n",
    "    def _get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        if tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        if tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        if tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        return wordnet.NOUN\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tags:\n",
    "        wn_tag = _get_wordnet_pos(tag) if tag else wordnet.NOUN\n",
    "        lemmas.append(lemmatizer.lemmatize(token, wn_tag))\n",
    "    return ' '.join(lemmas)\n",
    "print('[OK] Funções de pré-processamento definidas')\n",
    "# Aplicar pré-processamento\n",
    "df_train_proc = df_train.copy()\n",
    "df_val_proc = df_val.copy()\n",
    "df_train_proc['text_clean'] = df_train_proc['text'].apply(clean_text)\n",
    "df_train_proc['text_no_stop'] = df_train_proc['text_clean'].apply(remove_stopwords_en)\n",
    "df_train_proc['text_lemmatized'] = df_train_proc['text_no_stop'].apply(lemmatize_text_en)\n",
    "df_val_proc['text_clean'] = df_val_proc['text'].apply(clean_text)\n",
    "df_val_proc['text_no_stop'] = df_val_proc['text_clean'].apply(remove_stopwords_en)\n",
    "df_val_proc['text_lemmatized'] = df_val_proc['text_no_stop'].apply(lemmatize_text_en)\n",
    "print('[OK] Pré-processamento aplicado (dados em memória)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modelagem <a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 72306 | Validation: 999\n",
      "[MODEL] Treinando LogisticRegression...\n",
      "[RESULT] LogisticRegression — Accuracy: 0.8949 | F1-macro: 0.8917 | ROC-AUC(macro): 0.9804640563461514 | AP(macro): 0.9486615472017691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.87      0.85      0.86       172\n",
      "    Negative       0.87      0.94      0.90       265\n",
      "     Neutral       0.94      0.85      0.89       285\n",
      "    Positive       0.90      0.92      0.91       277\n",
      "\n",
      "    accuracy                           0.89       999\n",
      "   macro avg       0.89      0.89      0.89       999\n",
      "weighted avg       0.90      0.89      0.89       999\n",
      "\n",
      "[MODEL] Treinando MultinomialNB...\n",
      "[RESULT] LogisticRegression — Accuracy: 0.8949 | F1-macro: 0.8917 | ROC-AUC(macro): 0.9804640563461514 | AP(macro): 0.9486615472017691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.87      0.85      0.86       172\n",
      "    Negative       0.87      0.94      0.90       265\n",
      "     Neutral       0.94      0.85      0.89       285\n",
      "    Positive       0.90      0.92      0.91       277\n",
      "\n",
      "    accuracy                           0.89       999\n",
      "   macro avg       0.89      0.89      0.89       999\n",
      "weighted avg       0.90      0.89      0.89       999\n",
      "\n",
      "[MODEL] Treinando MultinomialNB...\n",
      "[RESULT] MultinomialNB — Accuracy: 0.7928 | F1-macro: 0.7862 | ROC-AUC(macro): 0.958353491194814 | AP(macro): 0.8934716097700365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.87      0.65      0.74       172\n",
      "    Negative       0.71      0.88      0.79       265\n",
      "     Neutral       0.88      0.71      0.78       285\n",
      "    Positive       0.78      0.89      0.83       277\n",
      "\n",
      "    accuracy                           0.79       999\n",
      "   macro avg       0.81      0.78      0.79       999\n",
      "weighted avg       0.81      0.79      0.79       999\n",
      "\n",
      "[MODEL] Treinando LinearSVC...\n",
      "[RESULT] MultinomialNB — Accuracy: 0.7928 | F1-macro: 0.7862 | ROC-AUC(macro): 0.958353491194814 | AP(macro): 0.8934716097700365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.87      0.65      0.74       172\n",
      "    Negative       0.71      0.88      0.79       265\n",
      "     Neutral       0.88      0.71      0.78       285\n",
      "    Positive       0.78      0.89      0.83       277\n",
      "\n",
      "    accuracy                           0.79       999\n",
      "   macro avg       0.81      0.78      0.79       999\n",
      "weighted avg       0.81      0.79      0.79       999\n",
      "\n",
      "[MODEL] Treinando LinearSVC...\n",
      "[RESULT] LinearSVC — Accuracy: 0.9379 | F1-macro: 0.9373 | ROC-AUC(macro): 0.988528382700553 | AP(macro): 0.9753868004488129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.94      0.93       172\n",
      "    Negative       0.92      0.96      0.94       265\n",
      "     Neutral       0.97      0.92      0.94       285\n",
      "    Positive       0.93      0.94      0.93       277\n",
      "\n",
      "    accuracy                           0.94       999\n",
      "   macro avg       0.94      0.94      0.94       999\n",
      "weighted avg       0.94      0.94      0.94       999\n",
      "\n",
      "[RESULT] LinearSVC — Accuracy: 0.9379 | F1-macro: 0.9373 | ROC-AUC(macro): 0.988528382700553 | AP(macro): 0.9753868004488129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.94      0.93       172\n",
      "    Negative       0.92      0.96      0.94       265\n",
      "     Neutral       0.97      0.92      0.94       285\n",
      "    Positive       0.93      0.94      0.93       277\n",
      "\n",
      "    accuracy                           0.94       999\n",
      "   macro avg       0.94      0.94      0.94       999\n",
      "weighted avg       0.94      0.94      0.94       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparar conjuntos X/y e remover vazios\n",
    "X_train = df_train_proc['text_lemmatized'].astype(str)\n",
    "y_train = df_train_proc['sentiment']\n",
    "X_val = df_val_proc['text_lemmatized'].astype(str)\n",
    "y_val = df_val_proc['sentiment']\n",
    "mask_train = X_train.str.strip().replace('', np.nan).notna()\n",
    "mask_val = X_val.str.strip().replace('', np.nan).notna()\n",
    "X_train = X_train[mask_train]; y_train = y_train[mask_train]\n",
    "X_val = X_val[mask_val]; y_val = y_val[mask_val]\n",
    "print(f'Treino: {len(X_train)} | Validation: {len(X_val)}')\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=2000, random_state=42),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LinearSVC': LinearSVC(max_iter=20000, random_state=42)\n",
    "}\n",
    "results = {}\n",
    "for name, clf in models.items():\n",
    "    print(f\"[MODEL] Treinando {name}...\")\n",
    "    pipe = Pipeline([('tfidf', TfidfVectorizer(max_features=15000, ngram_range=(1,2))), ('clf', clf)])\n",
    "    t0 = time.time(); pipe.fit(X_train, y_train); t1 = time.time(); train_time = t1 - t0\n",
    "    t0p = time.time(); preds = pipe.predict(X_val); t1p = time.time(); predict_time = t1p - t0p\n",
    "    acc = accuracy_score(y_val, preds); f1 = f1_score(y_val, preds, average='macro')\n",
    "    report = classification_report(y_val, preds, output_dict=True)\n",
    "    cm = confusion_matrix(y_val, preds)\n",
    "    classes = np.unique(y_val)\n",
    "    y_val_b = label_binarize(y_val, classes=classes)\n",
    "    y_score = None\n",
    "    try:\n",
    "        y_score = pipe.predict_proba(X_val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            decision = pipe.decision_function(X_val)\n",
    "            if decision.ndim == 1:\n",
    "                decision = np.vstack([-decision, decision]).T\n",
    "            y_score = decision\n",
    "        except Exception:\n",
    "            y_score = None\n",
    "    roc_auc_macro = None; avg_precision_macro = None\n",
    "    if y_score is not None and y_score.shape[1] == y_val_b.shape[1]:\n",
    "        try:\n",
    "            roc_auc_macro = roc_auc_score(y_val_b, y_score, average='macro', multi_class='ovr')\n",
    "        except Exception:\n",
    "            roc_auc_macro = None\n",
    "        try:\n",
    "            avg_precision_macro = average_precision_score(y_val_b, y_score, average='macro')\n",
    "        except Exception:\n",
    "            avg_precision_macro = None\n",
    "    results[name] = {\n",
    "        'pipeline': pipe, 'accuracy': acc, 'f1_macro': f1, 'roc_auc_macro': roc_auc_macro,\n",
    "        'average_precision_macro': avg_precision_macro, 'train_time_seconds': train_time, 'predict_time_seconds': predict_time,\n",
    "        'report': report, 'confusion_matrix': cm.tolist(), 'y_score': y_score\n",
    "    }\n",
    "    print(f'[RESULT] {name} — Accuracy: {acc:.4f} | F1-macro: {f1:.4f} | ROC-AUC(macro): {str(roc_auc_macro)} | AP(macro): {str(avg_precision_macro)}')\n",
    "    print(classification_report(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Avaliação <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Melhor modelo: LinearSVC salvo em: /workspaces/Senti-Pred/src/models/sentiment_model.pkl\n",
      "[OK] Métricas salvas em: /workspaces/Senti-Pred/reports/metrics/model_metrics.json\n",
      "[OK] ROC comparativo salvo em: /workspaces/Senti-Pred/reports/visualizacoes/comparison_roc.png\n",
      "[OK] Precision-Recall comparativo salvo em: /workspaces/Senti-Pred/reports/visualizacoes/comparison_pr.png\n",
      "[OK] Precision-Recall comparativo salvo em: /workspaces/Senti-Pred/reports/visualizacoes/comparison_pr.png\n",
      "[OK] Matrizes de confusão comparativas salvas em: /workspaces/Senti-Pred/reports/visualizacoes/comparison_confusion_matrices.png\n",
      "[OK] Matrizes de confusão comparativas salvas em: /workspaces/Senti-Pred/reports/visualizacoes/comparison_confusion_matrices.png\n"
     ]
    }
   ],
   "source": [
    "# Escolher melhor por F1-macro e salvar o pipeline\n",
    "best = max(results.keys(), key=lambda k: results[k]['f1_macro'])\n",
    "best_pipeline = results[best]['pipeline']\n",
    "joblib.dump(best_pipeline, MODEL_PATH)\n",
    "print(f'[OK] Melhor modelo: {best} salvo em: {MODEL_PATH}')\n",
    "# Salvar métricas em JSON\n",
    "metrics_out = {'best_model': best, 'results': {}}\n",
    "for k in results:\n",
    "    metrics_out['results'][k] = {\n",
    "        'accuracy': results[k]['accuracy'],\n",
    "        'f1_macro': results[k]['f1_macro'],\n",
    "        'roc_auc_macro': results[k].get('roc_auc_macro'),\n",
    "        'average_precision_macro': results[k].get('average_precision_macro'),\n",
    "        'train_time_seconds': results[k].get('train_time_seconds'),\n",
    "        'predict_time_seconds': results[k].get('predict_time_seconds'),\n",
    "        'classification_report': results[k]['report'],\n",
    "        'confusion_matrix': results[k]['confusion_matrix']\n",
    "    }\n",
    "with open(os.path.join(METRICS_DIR, 'model_metrics.json'), 'w') as f:\n",
    "    json.dump(metrics_out, f, indent=2)\n",
    "print(f\"[OK] Métricas salvas em: {os.path.join(METRICS_DIR, 'model_metrics.json')}\")\n",
    "# Gerar gráficos comparativos\n",
    "classes_all = np.unique(y_val)\n",
    "y_val_b_all = label_binarize(y_val, classes=classes_all)\n",
    "# ROC comparativo\n",
    "plt.figure(figsize=(8,6))\n",
    "plotted_any = False\n",
    "for name in results:\n",
    "    y_score = results[name].get('y_score')\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_val_b_all.ravel(), y_score.ravel())\n",
    "        auc_val = None\n",
    "        try:\n",
    "            auc_val = roc_auc_score(y_val_b_all, y_score, average='macro', multi_class='ovr')\n",
    "        except Exception:\n",
    "            auc_val = None\n",
    "        label = f\"{name}\"\n",
    "        if auc_val is not None:\n",
    "            label += f\" (AUC={auc_val:.3f})\"\n",
    "        plt.plot(fpr, tpr, lw=2, label=label)\n",
    "        plotted_any = True\n",
    "    except Exception:\n",
    "        continue\n",
    "if plotted_any:\n",
    "    plt.plot([0,1],[0,1],'k--', linewidth=0.5)\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('Comparative ROC Curves (all models)')\n",
    "    plt.legend(loc='lower right'); plt.tight_layout()\n",
    "    roc_path = os.path.join(VIS_DIR, 'comparison_roc.png')\n",
    "    plt.savefig(roc_path); plt.close(); print(f'[OK] ROC comparativo salvo em: {roc_path}')\n",
    "else:\n",
    "    print('[WARN] Nenhum score disponível para plotting ROC comparativo')\n",
    "# PR comparativo\n",
    "plt.figure(figsize=(8,6))\n",
    "plotted_any = False\n",
    "for name in results:\n",
    "    y_score = results[name].get('y_score')\n",
    "    if y_score is None:\n",
    "        continue\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_val_b_all.ravel(), y_score.ravel())\n",
    "        ap = None\n",
    "        try:\n",
    "            ap = average_precision_score(y_val_b_all, y_score, average='macro')\n",
    "        except Exception:\n",
    "            ap = None\n",
    "        label = f\"{name}\"\n",
    "        if ap is not None:\n",
    "            label += f\" (AP={ap:.3f})\"\n",
    "        plt.plot(recall, precision, lw=2, label=label)\n",
    "        plotted_any = True\n",
    "    except Exception:\n",
    "        continue\n",
    "if plotted_any:\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Comparative Precision-Recall Curves (all models)')\n",
    "    plt.legend(loc='lower left'); plt.tight_layout()\n",
    "    pr_path = os.path.join(VIS_DIR, 'comparison_pr.png')\n",
    "    plt.savefig(pr_path); plt.close(); print(f'[OK] Precision-Recall comparativo salvo em: {pr_path}')\n",
    "else:\n",
    "    print('[WARN] Nenhum score disponível para plotting Precision-Recall comparativo')\n",
    "# Confusion matrices lado a lado\n",
    "model_names = list(results.keys())\n",
    "cms = [np.array(results[nm]['confusion_matrix']) for nm in model_names]\n",
    "if len(cms) > 0:\n",
    "    vmax = max(cm.max() for cm in cms)\n",
    "    fig, axes = plt.subplots(1, len(model_names), figsize=(6 * len(model_names), 5))\n",
    "    if len(model_names) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, nm, cm in zip(axes, model_names, cms):\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes_all, yticklabels=classes_all, vmin=0, vmax=vmax, ax=ax)\n",
    "        ax.set_title(f'Confusion — {nm}'); ax.set_xlabel('Predito'); ax.set_ylabel('Real')\n",
    "    plt.tight_layout(); cm_path = os.path.join(VIS_DIR, 'comparison_confusion_matrices.png'); plt.savefig(cm_path); plt.close(); print(f'[OK] Matrizes de confusão comparativas salvas em: {cm_path}')\n",
    "else:\n",
    "    print('[WARN] Nenhuma matriz de confusão disponível para plotagem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deploy com Docker <a id='deploy'></a>\n",
    "(Se desejar, execute as células abaixo para gerar artefatos Docker/README para deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: criar Dockerfile e docker-compose para API (simplificado)\n",
    "dockerfile_content = '''\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "EXPOSE 8000\n",
    "CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n",
    "'''\n",
    "with open(os.path.join(BASE_DIR, 'src', 'api', 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "print('[OK] Dockerfile criado (src/api/Dockerfile)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
